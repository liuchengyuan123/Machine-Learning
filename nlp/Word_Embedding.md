# word embedding

### 我在task1中的训练模型

在task1中，我曾经尝试使用我最原始的想法对情绪判断进行训练。

- 统计所有的单词，为单词编号，记录为$one-hot$编码。
- 对一句话中出现的目标词汇，生成独热编码，进行神经网络训练。
- 每句话的标签为“积极”或“消极“之一
- 激活函数为$sigmoid$
- 损失函数为交叉熵

其中，有两种特征提取方法，输入向量可以为一各单词的独热编码，也可以是一个词组的向量，即

- 词袋
- N特征

我对这两种方法分别进行了实验。

词袋法正确率随着训练增加逐渐提高。

N特征法的效果却不尽如人意。

- 局限性

  使用这两种方法都有一个显著的缺点，即输入的稀疏矩阵运算效率极低，为了权衡速度，甚至要减少词袋大小和向量的数量（在遇到陌生单词或陌生词组的情况下无法识别）

  这种训练方法根本没有注意到单词与单词之间的联系与区别，使得识别效率极低。

### word embedding

实际上，可以把独热编码表示的单词用一组稠密的向量来表示。这组向量不仅起到**降维**的作用，还应该能够**表现属性相似或相反的单词之间的关系**。

#### 神经网络语言模型

我们假设存在一个矩阵$W$使得我们可以通过独热编码映射得到我们现在要找的那个单词向量。

我们的这个embedding后的单词向量还可以反映它的词性等特征。在task2中，这种性质是通过上下文的属性来体现的。

构建这样一个神经网络：

- 独热编码通过一层带tanh的网络变成词向量
- 词向量再经过一层网络匹配上下文环境的独热编码
- 不断训练这两层的参数
- 优化目标是输出的编码与真实的上下文环境的差距

#### word2Vec

实际上这个模型的复杂度还是蛮高的。
主要庞大的计算量在后面。

word2vec是一种常见的embedding方法，相比上面的模型，word2vec将产生的词向量直接在最后一层softmax上面跑。我们真正想要的，并非最后产生的一致性真的很高，而是中间的副产物——词向量。所以其实更应该关注第一个转移矩阵。

##### CBOW

选择一个大小固定的框，这个框中有一个target，是网络的预测输出，这个框内其他的位置都是预测的source。如果给定上下文的出现概率情况，那么我们通过整个网络应该可以预测中心词target为各种单词的可能性，最小化的目标就是用交叉熵衡量的预测概率和真实的概率（target的独热编码）的差。

CBOW按照上面的说法实现的。下面是我的实现过程：

- 统计所有单词，创建$one-hot$
- 创建数据集、标签（包含用一个滑窗统计单词等一大堆步骤
- 训练模型，第一层在产生词向量的时候没有激活函数，第二层softmax，loss为交叉熵。

<font color="red">然而由于未知原因，我的实现还有错误，尚未解决</font>

##### skip_gram

之前考虑的都是用上下文预测中心词，skip_gram则是用中心词预测上下文，实现与CBOW模型正好相反。
