# transformer

### 概述

transformer模型可用于机器翻译等场景，也是encoder-decoder结构，它由若干个encoder和若干个decoder组成。

#### 编码器

所有的编码器结构都是相同的，可分解为两个子层，自注意力层和前馈神经网络。self-attention可以帮助编码器关注输入句子的其他单词。

#### 解码器

解码器有和编码器类似的构造，不同的是，它在self-attention与前馈神经网络间还有一个attention，用来关注输入的句子部分（与seq2seq的注意力相似）

![img](https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=1956828140,2631922143&fm=173&app=49&f=JPEG?w=640&h=196&s=8570E53285B0442350D171C0030070B2)

------

### transformer

#### 预处理

word-Embedding。

输入序列的每个位置的单词都有自己独特的路径流入编码器，在自注意力层这些路径间相互有依存关系，前馈神经网络层则没有。

#### self-attention

计算自注意力的第一步就是从每个编码器的输入向量中得到三个向量$q, k, v$，这三个向量分别是由输入向量和三个权重矩阵相乘得到的。

接下来我们用矩阵表示自注意力的计算过程。

假设$X$为输入的词嵌入向量。
$$
Q = X \times W^Q \\K = X \times W^K \\V = X \times W^V
$$

$$
Z = softmax(\frac{Q \times K^T}{\sqrt{d}}) V
$$

#### “multi-headed” attention

多头注意力机制完善了之前的self-attention，比如在很多情况下，一个单词的词义可能受限制于多个单词，那么我们都要关注，“multi-headed” attention就可以帮助我们解决这个问题。

<font color = red>它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。</font>

单独计算8各注意力头，我们会得到8各不同的$Z$矩阵，为了同时考虑他们的同时，还要匹配前馈神经网络，我们的做法是将8各$Z$拼接起来，然后再用一个权重矩阵$W_O$和他相乘作为前馈神经网络的输入。

#### 为模型添加顺序

之前我们所论述的模型是不考虑单词顺序的，即”我打了你“和”你打了我“是完全一样的，但这显然不合理，我们应当考虑单词顺序，或者说单词距离的远近。

为了引入单词顺序，在模型的最开始输入层，用原来的输入$X$加上一个额外的位置向量$T$，使得输入的向量能够携带我们的位置信息。

#### 残差模块

在每一个自注意力层、前馈神经网络层周围都有一个残差连接和归一化层。

![img](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=1082514457,203002629&fm=173&app=49&f=JPEG?w=640&h=585&s=0842EC12599FC0C854F94CD90300D0B2)
